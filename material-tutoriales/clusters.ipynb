{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Clustering jerárquico"]},{"cell_type":"markdown","metadata":{},"source":["### Clustering Jerárquico - Aglomerativo\n","\n","Vamos a ver una técnica de clustering, que es el <strong>Clustering Jerárquico Aglomerativo</strong>.\n","Recordemos que esta estrategia parte de un conjunto de elementos individuales (donde cada elemento es su propio cluster) y se van juntando los elementos que más se parezcan hasta quedarse con un número de clusters que se considere óptimo.\n","\n","Imagina que quieres separar un grupo de 4 personas, entonces con esta estrategia tomas a las personas individualmente. \n","\n","Supongamos que tienes la información de que las personas 2 y 3 viven en el mismo país, esto hace que las juntes en el mismo grupo (las personas 1 y 4 viven cada una en un país diferente).\n","\n","Luego descubres que la persona 4 habla el mismo idioma que las personas 2 y 3, por lo que puedes añadirlas al mismo grupo (por ejemplo, \"personas que hablan inglés\").\n","\n","Por último, te das cuenta de que la persona 1 vive en Europa, mientras que el resto de las personas viven en América. Esto coloca automáticamente a esa persona en otro grupo. \n","\n","<img src=\"https://crisdavid3335.github.io/img/clusters_jerargicos.jpg\" width=\"400\" align=\"center\">\n","\n","Ya has agrupado a estas personas en diferentes niveles, así es como funciona el algoritmo. Parte de características que, cuando se comparten con otros miembros, permiten generar grupos. Cuanto más generales sean las características, más grandes serán los grupos.\n","\n","Ahora veámoslo en código:"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["# Primero importamos el las librerias necesarias:\n","\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from scipy import ndimage\n","from scipy.cluster import hierarchy\n","from scipy.spatial import distance_matrix\n","from sklearn import datasets, manifold\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.datasets import make_blobs\n","\n","%matplotlib inline\n"]},{"cell_type":"markdown","metadata":{},"source":["Vamos a generar un conjunto de datos utilizando la clase make_blobs.\n","\n","La clase make_blobs toma los siguientes parámetros:\n","\n","<li><strong>n_muestras:</strong> El número total de puntos divididos equitativamente entre los clusters. En este caso usaremos unos 250 puntos.</li>\n","<li><strong>centers:</strong> El número de centros o centroides a generar, o las ubicaciones fijas de los centros. En este caso usaremos 4 centros fijos con las siguientes ubicaciones ([4,4], [-2, -1], [1, 1], [10,4]).</li>\n","<li><strong>cluster_std:</strong> Es la desviación estándar de los clusters. Cuanto mayor sea el número, más separados estarán los clusters. Esta ves usaremos la desviación de 1.2.</li>\n","<br>\n","Finalmente se debe guardar los datos y sus etiquetas de forma (datos, etiquetas)."]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["# Generamos los datos y las etiquetas:\n","data, labels = make_blobs(n_samples=250, centers=[\n","                          [4, 4], [-2, -1], [1, 1], [10, 4]], cluster_std=1.2)\n"]},{"cell_type":"markdown","metadata":{},"source":["Pongamos estos datos en forma visual, en un gráfico de dispersión:"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAlIAAAFECAYAAADsuNBoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnzUlEQVR4nO3df7wddX3n8fcnBIhoAkQUrkJIqqzdWKviqZFiFYrlV1F0123o+gv1scHdB13d6iLIrpvWh27dPmzB1takVKtd1HS1FuoDRFDZLiVEb1CMXqT8Ckq8CDbCDdKA0c/+MXOSycmcX/PzOzOv5+NxH/fec+bMfOc7c2be5/v9zhxzdwEAAGB6i+ouAAAAQFMRpAAAADIiSAEAAGREkAIAAMiIIAUAAJARQQoAACAjghSAzjGzG82ssnu/mNl6M3MzOyXlud82s2+Y2a54msvix7eb2faqygggG4IUUKP4xJn8edzMHjKzW83sCjM7y8wOKmhZ58fLOL+I+SE/MztJ0pWSlkr6c0m/J+mLtRYKwFQW110AAJKiE6gkHSTpCEnPlfQGSW+VNGtmr3P3f6qpbMjvTyV9RtL3Bh7/TUkm6Y3ufvPAc6dVUTAA+RCkgAC4+/rBx8zsaEl/IunfSbrBzHru/mDVZUN+7v4jST9KeeoZ8e8fpLzm7lILBaAQdO0BgXL3H0o6T9KNko6T9J7k82b2IjO73MxuM7OdZrbbzO40sw+Z2ZED094o6ePxvx8f6E5cmZjucDP7n2Z2Rzy/H5vZdWb2isHyWeRNZnZz3B2528y+H0+/dpJ1NLNnmNl7zewfzewBM3vCzH5gZp8ys9VTVFd/fsvN7P1m9m0ze8zMHonr5w/M7MljXnuImV1oZteY2X1xN+tOM7vBzM4a8ppfNrNPx+OZkt2yl5nZwYnp9hsj1e9mlfTmeJJ7B7fHqDFSZrbWzL6c2O7b43L0EtMcbmb/1cy+Ymb3x3X7kJldHXcpps3X4/Fjx8RdyzvM7Gd0BwPDGd+1B9SnP+DZ3W3ENKdJukHSg5KO8fhNa2YflfQaSf9X0vcVfTB6kaRfk3S7pDXuviue9nxJr5Z0rqSrJH0zsYjL3P1hMztC0j9KWi3p65K+IukoSb8l6SmS/qO7b0iU6wOSLpF0r6RrJT0iaUbSr0j6rru/doL1P0/SxyR9VdJ2SY9KOkHSOZKekHSyu982bj7xvFbF8zle0ta4XhZJ+leSXiHpOe6+PZ72RkkvT9a7mR0jaYekmyXdIemheH1eKWm5pP/g7lckpv9lSVskuaSr43pYJunZkk6VtNzdH42nXS/pf0g61d1vNLMXKNoer5b0fEmXS3o4nnV/e2yXJHdfmVimKQrEb1LUwnVVXM5j42Ve0W/dNLOXSPqH+OduST+WtELSqyQdKumV7r7feKx4f9wWr8ejcX3+XNIX3f3aoZUPdJm788MPPzX9KDoJ+5hpDpX003jaVYnHj5d0UMr0b42nfffA4+fHj58/ZDkb4uc3KP6QFT9+gqKQ9LiklYnH/1nS/ZIOS5nXUROu/9MlLU15/PmKTuTXTlGXN8flvyStPJKWJP6/cbDe43o+NuW1h0v6tqSdkp6UePxD8fLOTXnNkZIWJf5fH097ysB0fxU/vjJlHtslbR94bF08/dckHT7w3EGSZgbKfcB2UBS6fiDp9mH7o6RPSlpc9fuBH36a+EPXHhA4d39cUWiRpKclHr/P3X+W8pKPSVqQdMakyzCzQyS9XlF4ucTd9zZVu/udkj4s6RBJbxx46U8lHVAGj8YEjeXuD3rcajbw+G2KWsROTXaRjSj/iySdpKil7YNp5XH33WPK8ri735/y+COK6vRIRa1tg/4l5TU/dvefjyt3Br8T/74gLldymT9z9/nE/4+kbYd4HT8r6RfNbEXKMp6Q9C5331NguYHWIkgBzdDvgtobcMzs4HhMz03xWJmfxV0zP1fUNfPMKeb/HEmHSbrN3XemPP+V+PcLE49dKWmlpLl4XNWZZnb4FMvsr8dvmtnfm9m8mf20P1ZIUZfaoYpak8Z5Sfz7ujwBxsyea2Z/ZWb3mNm/JMryoXiSZJ1uUhQi/87MPmlmbzSzZ2Vd9gRle7KkX5L0Q3f/xoSvOdnM/iYeu/Z4Yn36gSxtH9nuXNQATIyr9oDAmdkSRWN0pGg8TN8mRWOk7lE0VuYBRd1vkvQORSFkUv0AND/k+f7jRyQe+y/xst8s6eL4Z4+ZXSPpne5+17iFmtnbJV2maPzO9YpuD/CYosD4akVdfJOsR79cOyaYdlhZXqIoMC6W9GVF454WFAXTFygaX7a3LO7+NTP7NUmXSnqtottVyMzukPR77v7prGUZ4oj490TraGavUdTytFtR3d4t6SeK1ucUSS9Xet0+kLOcQKcQpIDwvVTRe/WHvm+wdE9RiLpB0lnJbhgzWyTpoimX0e8mOmbI8zMD0ynuVrxM0mVm9vS4nOcpul3Dc83suXG3ZCozW6xo7NADkk5MdkvFz6deWTbEw/HvaVrhBv03SU9SPCB8oCyXKApS+3H3zZLOMbNDFQ30P1NRa8+nzOwhd78hR3kGPRz/nnQd36eom67n7rcnnzCzDYqCVBquQAKmQNceELA4FF0a//upxFPPjn9fnTKW5cWKAsGg/limtDul36GoJej58dV7g06Nf9+aVs54rNPfuvtvKWrVeZaibqhRjlLUynJzSoh6iqQTx7w+6Zb49xlxnWXxbEk7B0NUbFjokLR3fNXN7v5eSf85fviA4JWHu/9E0aD3o83sheOmV7Q+cykhapGi0AugAAQpIFBxK89nFHXDfE/SBxJPb49/n5Lymo8MmWV/wPoBA4zd/Qnt+6qS9w3M81mKwsFPJf11/NihZnZySpkP1r5uyMeGlKPvwXiaF8XBKTmPyzXZ2Kh++bcqumrvBZLenVKup8ZdpKNsl7Q8vq1B8rVvVcrAfTP7VTNLC6xHx7/HrX8WH45/bxgcj2Zmi8xsJvHQdkknmNkzEtOYolbAqe/RBSAdXXtAAOL7DEnRh5sjFH1FzEsVXSn3NUmvG7gC6+uK7vn0b8zsZkk3KTqBn6WodemAO2VL2qzo5P4OM3uq9o2F+ZP4CrCLFd2D6kIz+xVF9xDq30dqqaQL3f3e+DVPknSTmd2l6J5N90laIuk3JP1rRS1l+7WEDHL3n5vZh+PlbjOzq+L1PVVRGPuq9rWETeL1im5r8AEz+7fx36bo9g2nS/pF7QugaS5TFJhuMrO/UdSN2VO0HT6raBxU0kWSft3M/p+ie0g9qmi7naVozNfGKco+qSsUbaM3SLozrrOHFN0h/dcVXV24Pp72jyV9VNI3zOxzioLwyYpC1N8rGswPIK+677/ADz9d/tG++/b0fx5XdKPFrZL+QtGYm0VDXrtc0p8pCge7FQ0m/oCiq++2a+AeRPFrzlQUqB5NLHNl4vkjFN0+4M64LA8rGqh8+sB8DlYUJK5V1Fq2W9EJ/RZJb5N0yITrv1jS70qaU3QbgQcUtXodrxH3WBoxv6fG5b8jLtPDim6J8H4l7nellPtIxY+fE6/Drvi1X5L0MqXcg0tROPt4XPZHFA3kvkNRq9HxA/NdrwLuI5V47nWKbjj6SLye9ypqUTxxYLrz4/X/SbxffV7S80aUxyXdWPf7gh9+mvTDnc0BAAAyYowUAABARgQpAACAjAhSAAAAGRGkAAAAMiJIAQAAZFTLfaSOOuooX7lyZR2LBgAAmMrWrVt/5O5PS3uuliC1cuVKzc7O1rFoAACAqZjZfcOeo2sPAAAgI4IUAABARgQpAACAjAhSAAAAGRGkAAAAMiJIAQAAZESQAgAAyIggBQAASrF2w2at3bC57mKUiiAFAACQUS13NgcAAO3Vb4Xacu/O/f7fdMFJtZWpLLRIAQAAZESLFAAAKFS/5anNLVF9tEgBAABkRIsUAAAoRZtbovpokQIAAMiIIAUAAJARQQoAACAjghQAAEBGBCkAAICMCFIAAAAZEaQAACjANF/Q24Uv8+0KghQAAEBG3JATAIAcpvmC3i59mW9X0CIFAGgcusYQClqkAADIYZov6O3Sl/l2BUEKANAYdI0hNAQpAEBjzc0v1F2EvaYJcwS/9iBIAQAaY7BrDKgbQQoA0Dj9lqhdu/dIoosP9eGqPQBA46yeWabVM8vqLkZQuJKxHrRIAQAah6vfEAqCFAAADcaVjPUiSAEAGouwgLoRpAAgJ1oAUCe6OevFYHMAmAIDelE09qlmo0UKACa0dsNmzc0v7L1ajLEpCAn7XT0IUgAwgX6I2rV7j7bcu/OAUAVMiyDeDoUEKTM7QtIVkn5Jkkt6i7vTTglUiINweZIhqq8fojZdcBJ1D3RYUS1Sl0v6oru/1swOkXRYQfMFgCCsnlm2t+Vg6ZLFe0MUkBWDxNshd5Ays8MlvUzS+ZLk7k9IeiLvfAFMhu6B8iVPeMmWqMHn0T0hvN9CKEOXFdEitUrSQ5I+bmbPl7RV0tvd/ScFzBsAgkJLFIoWwv5EGMvO3D3fDMx6km6RdLK7bzGzyyUtuPt/H5hunaR1krRixYoX3XfffbmWC2B/bToQtmld0Fyj9sPBluA1q5YPnbYsRZah6Pdc297DZrbV3XtpzxXRInW/pPvdfUv8/2clXTw4kbtvlLRRknq9Xr70BgAAcmNoQH65g5S7P2Bm3zez57j7HZJOkzSXv2gAptGGA18ZB3VODJjWJPthCAPFQyjDoC4Gs6Ku2vsdSVfGV+zdI+nNBc0XAAAkFBlOQgxjTZN7jFQWvV7PZ2dnK18ugGYosiUq6/gRTiwIdR8oo1yMkRqt7DFSAACgZGV2m7Ul8NSBIAUgOHV2WXRxjAfSsc2z61LdEaQAoIEIeN3DeKYwEaQAtMaoq6smxckKwDQIUgDQIHQ9YpJtzX5RHYIUgMabNFxMc3LhBARgEgQpAGgQuh4xStktlux3ByJIAWi8ceGiju4wTjhANxCkADRa0wNL1vI3dX1RrrJaLBmbNxxBCkBrDDuoV9kdxgkH6BaCFIBGanpgaXr5Ebai9yPG5g1HkALQGVUc/DnhQGL7dwlBCsBIoZ4Qmh5Yml5+dBP3sDoQQQoAStCVkwj2R5dt9xCkAKRqygkhtPJMq+nlR/OU9V5uyjGjaAQpAAAK0sQu2yaVNUQEKQCpmnhCAJquzPfbsBajonT1mEGQAgCgYE0IEXPzC5KkXbv3SGpmAAqhzAQpoGOmPfA06aA6KISDLDCJKsYXDbYY9fWXWfRyuoIgBQBAh/RboratP0NSMz9whDSwnSAFdERIB56ydWld0Q5Vji9aPbOstHl3EUEKAIAOaNMHjJAGthOkgI4I6cBTtirWtQv1WKeq6zeU7Vn38jE9ghQAoHNCCU5VauOHqRDWgSAFdEwIB56qVHkvni7Va5mqrl+2J/IiSAEAOmMwOC1d0r3TIF3dxereHgQAOYTcPRJimaZVdf0OXsGWd3lt2AZtWIcqEaQABCPrAZwDPyaVDGr9+yltuuCkzF+X0vV9j65RghSAFivzoD5unlWeUNp4Miu67OPqpKiWqCZvgzasQx0IUgAKN+0BOOsBfNrXcWJAvyVq9cwy7dq9R1vu3Zlpv5h0rFXb97mQu7qrQpAC0Dplf8t9lmWXeYLhZDZcWuB57PE9U79+XJ0OG2tV5b6XF/tRNgQpAIXJGiKyHsDTXpdscehrw7fcoxj9fUCKQtXqmWWZ9oNh+2z//7q6yerat7v8XiJIAWid/smx7G+5T1Pnp/oun8yG6dfJ89ZfJ2n/IDVKUR8K+vphvgnYj6ZDkAJQmLwhIusBvB+a1m7YvN+Jr98yRZcFir7Nwbj5VN21x0Dx+hCkALRW1m6bQcO6b0bNmxNYWIZ1+46aXsofSKroVp5mvYa9ftoyEdT2IUgBKFwdB9dJTnwc9DFtuM7bJdcPN2V3Kw92Z7OvV8fcvZgZmR0kaVbSDnc/Z9S0vV7PZ2dnC1kuACQVeSIZdol7v3VhzarlhS2rTm0/+Q5ux2m2W1oXcZ4yFN0SJe2/XtOUNcvr89Rl8vVN29fMbKu799KeK7JF6u2SbpeUrW0RAArQtAM0wjQYGObmF7R2w+b99q8QQ0FR3dmYXCEtUmZ2rKRPSHq/pN+lRQrAJEI8EaXJMkaqCfK2LjTNNNut3xI16nYJoewHecuRbHWbdF/Ie9Pdpu1rVbRIXSbpIklLC5ofgIBUccLoX56+bf0ZmV4/rIyhnOzQLMnB6f0w1R/vxBVySModpMzsHEkPuvtWMztlxHTrJK2TpBUrVuRdLIDAjTq5pJ2IHnt8jw47dPghqc6TVVtPkKG1rJQtyzieUbfRCEXe7ZaluzLrrU3auK8V0SJ1sqRXmdnZkpZIWmZm/9vdX5+cyN03StooRV17BSwXQMmq+uQ9u32nfubRIO5pW6bGfR1MkWUP7QSK8qXd2HXwJp9tCgWYXu4g5e6XSLpEkuIWqXcNhigA3TFJ+Ep+Op3dvlOHHbp46B2nQ+pGaetXzTS9/EWZdF/rP962/UAqfx3aUEeDuI8UgKGqaI7vrVyuTReclHmM1LgyFtkSNenXiwDojkKDlLvfKOnGIucJoFny3P37scf37HeJ+eBYlXHzq0r/flIhlAXFmXTfbfN4H0yPFikAQ1V5oti2/oxcY5DGnfTyCHWQcRcQVhA6ghSAiU1zUsvyvV39sSn9br5Qx6BMc9foUMo8KPTy1WnSOqHuIBGkgEo15eQ17kq4rqJLpzrTXGTA9kCdCFIAxir7arVhAaWJJ8i01rWQvrYjpKsggTYgSAEVaNrJa9iYoLK/wT50TduOTTZJmGZ7dHOdQ0OQAjBWVS1Eg/Nt4smhf4Xh0iXRvbGSrXghXH3Y5Na+NmN7NBdBCqhAU09eTSlnVZq6HZtskttndHF70BoXDoIU0FB1HDg5SE9m2H2w1m7YHMyJj20ZBgJR8xGkgApxcGwHtmNYurg9utwaFxqCFNAwfIJtjmHfLzj4HLqL/aL5CFIAAORQZwgieNWPIAU0DJ9gm43thTTsF81FkAIahruMA2Ggmx0SQQpoLA7W7cEJGGgughTQEHz6BcJCNzskghSAAnFCmQ7hGGg+ghTQEHz6BcLEe7HbCFIAcpu0ZYUQuL+uhOO2r1/bsL2mQ5ACGoaDGwCEw9y98oX2ej2fnZ2tfLlAEfi0Nty4lqh+i9WaVctTp0O7sN2bhe01nJltdfde2nOLqi4MgDD0v0Q3dMlyNqXMQNV4b9SHrj1gQqPGAdFKFRm2/l0ZC4T9sd2bhe2VDUEK6JimXHI/WM7nrb9Ou3bv2e+50Mo8iLCNsjXl/dxmBClgQmmf1vrN6V06iOVZxxDrpQvbrG7UbbOwvaZDkAI6pinN98OCa/K5ULWhNQ3N0JT3c+jly4MgBUwpeSBoykGsCG3rQmjb+mAyVW9n9qv2I0gBHdWUA3tacA1dk1vT0Eyh7ldd+MBCkAIK0KaDwjBta31r2/pgtCJP6JO8tgsBAhGCFIDGCunklFaWJramAUXqwgcWghSAqbTtQFjm+rT55NE0RZzQp2ll6kKAQIQgBaBxQuo2CaksQKja/H4gSAFAwebmFzp3f7GmyLMNsrQysc3bjyAFFIATZbVC6jYZdYUegPYjSAEN1fb74YQQkrIKKeiheGxPJBGk0ElFneAmGR/DybQ8IdVpSGUBhuF4VDyCFIJR9n1dQjVt2ase3NzE5YWyP9S9fADlyx2kzOw4SZ+UdLQkl7TR3S/PO1+gDEWHglFdOFzNBSAUHI/KU0SL1B5J73T3W81sqaStZna9u88VMO9M2EGaJc8bvMkHh6xlr3r8TZOW1+T9AUAz5Q5S7j4vaT7+e5eZ3S7pmZJqC1LAMGWFAm7IByBkHI/KU+gYKTNbKemFkrYUOd9J8Wm0mfK8wZt8cMhb9qrXtQnLa/L+AKCZCgtSZvYUSZ+T9A53X0h5fp2kdZK0YsWKohYLZFLlCZaT+eQIQEC5eG8Vz9w9/0zMDpb0BUnXufsfjZu+1+v57Oxs7uUOw8EYaCbeuwBCZGZb3b2X9lwRV+2ZpL+UdPskIQoABtEtD4SH9+FkiujaO1nSGyRtM7Nvxo+9x92vKWDembDRgepwsAXQZUVctXeTJCugLAA6ikHiQDhoIZ4OdzYHkMk0B1sOxADaiiAFpODEXw/qG6gfLcTTIUihVXjjj1dUHU1ysKWLAEDbEaSABE78ABDhuDcZghRaIUsA6lpISqujufkFrZ5ZlqsORr22ji6Crm1XAPUiSAEJjA0AAEyjkDubT6vsO5uju6Zpieq3zKxZtfyA10wTpJoWupItUaPqoGkm2a4AkEWpdzYH2oiTLwBgErRIobPytCQ1oVWnq+PE2rhOAOo1qkVqUdWFQfOs3bB578kJAADsQ9ceOitrS5S0bxyOJC1dsjj3lW9FmubqvFDKXKQ2rhOAcBGkMBT3VAIAYDSCVAcQgIrThNsjJMs4N78gSdq1e4+23Lsz6HIDQBMRpDBUE0JDk1CPANA+BKkWo2uuPHXV4TTbcNh9sfoXD7AfAEB+BCmMxQk3HwItALQXQarF6Jprj7xhLNkSRaADgOIQpIApZAkfBNrqFV3XbDsAwxCkOoCDf/MVEcbKCHQEDABdR5Bqma6e2Mpe7yLGOXVtm9Sh6PFojG8DMA5BqgIcfIcLoW5CKMOkiihjkS1RBAwAXUeQaom2ndgmLX9V6804p2Yoejux3QGMQ5AqUdvCzaTGrWfyu9/qrJsyt8+weRW1jCoGU49aBgEDACIEqZbIcmIL8SQ4bbip+oQeUl1huLJaJAFgEEGqRF27SmpcCBp8fm5+QQeZ1Fu5/IBpqlDG9nne+uskRd9tl5x3X97WryoGU0/TWhjifggAVSJItcw0LVEhdjlmDTfcL+hAbVoXAAgVQaoCdV8lVXW317Dl9f8fbLXpv6auE36R26e/TkuXLE6dd97vvKtyMHXIQSzksgHoFoJUBzVhoHDVZQq5lW5abVoXAAiduXvlC+31ej47O1v5ctsgS0tU/4S6ZtXy/V5b9wm27uUnjaurtGmnfS7LsrIoe/51avO6AdMK6Rjadma21d17ac/RItVhTXzzcZ+o8dq0LgAQOoJUw1QxnoZbCaSbpMtsknUi6OyT9aIC6g5dRvd9WAhSaISq72BehKoObsNuM8FBFQDKR5BqsElPmNO2RPEpJ11ZV8x10eC+1r+Sc9v6MyZ6fZfrDuBDU1gIUmiEOg8cWbtGyw6kw8LI4M1AOcgCQHkIUg1U1om6yZ9yqixzk+olVMPuKdbEfQ+oC++TMBCkWq5tJ6YmfKlxVYF02HLats0BIGSFBCkzO1PS5ZIOknSFu/9BEfNFurJPmEV+59ykY16yYlxXs/X3D7YbgKbKHaTM7CBJH5H0G5Lul/R1M7va3efyzhvZETDyyxtY89Z11osJ2MYAUJ0iWqReLOkud79HkszsM5LOlUSQKlmIJ8zBMS9lt0zRndUObDcATVVEkHqmpO8n/r9f0poC5oscCBjFCfV7/9i2AFC/ygabm9k6SeskacWKFVUttlHacGLstzyV0RLVr5++ae8oDgBA0YoIUjskHZf4/9j4sf24+0ZJG6XoS4sLWC4mQMBonsFbAwxriWL8GwDUr4gg9XVJJ5jZKkUB6jxJ/76A+XZGG0+MaS1RWddrsH7yzg8AgKLkDlLuvsfMLpR0naLbH3zM3b+Tu2RAhyRDYf/vYTepZPwbAISjkDFS7n6NpGuKmFcXhXpiLKo8eVvcButn8HEAAOrCnc1RirJC4dz8giRp9cyyQudbl1Ehc1wdEiQBoH4EqYCEcmIsesxWUS1uk4SLNpibXzig9Q0AECaCFApV1sD5Ng7IT1qzarmk7oRFAGgLghQOUNaYLYLBaP2WqEnCYtPCVtPKCwCTIkgh1doNmzU3vzD1WKSyQ1hbTsiDLWz9FikAQLMQpLDXYEhZPbOs8YElqzoCW7++J2mJakoXZ9PKCwDTIkhhP9N0L41S1omyLSfgtrWwAUBXEaRwQKtB/xYDXRRCC8qoZTUtgDWtvAAwLYIUDrB6Ztne8VGc+MpF/QJAs5l79d8f3Ov1fHZ2tvLlYrRpbgbZdl1ffwDAPma21d17ac/RIoVUBAgAAMajRQpj0ToDAOiyUS1Si6ouDAAAQFvQtYehQriCDQCAkNEihVKs3bCZL94FALQeLVIlCrkFZ5Ky1XkPoJDrDgCAPoIUUmUNMnQHAgC6hCBVgpDDRJay1dESFWLdAQAwiCAVoDrDQ94gw1eCAAC6hCBVgpDDRMhlk8IvHwAASQSpgITQrVVUkCEAAQC6gCBVojq75sYtO+35soJblvkSxAAATUCQCkhI3VoEGQAAxiNItUSebsGyuhRD6KoEAKBMBKkAlRk02hhm2rhOAIBmIEi1RJ5uwbK6FEPqqgQAoAwEqQqEECTa2M3WxnUCADQLQapl8oSIaV47TWgh2AAA2oogVaKQWkxC6mYrqgwhrRMAoJsIUphKSOEQAIC6EaRKFGKLSZ03Ce0rOoSFUK8AgG4iSGEqk4bDkMIjAABlIUhVIMQwUUXQmZtf0NoNm/e2QK1ZtVxz8wtaumSxVs8sC7JeAACYBkEKmYxrieqHp7n5hcrKBABA1QhSNaqj+6vIweKTvHb1zLL9wtSu3Xv2ez2tUgCAJltUdwHQLpsuOEmbLjhJa1Yt15pVy7XpgpO0emZZ3cUCAKAUuVqkzOwPJb1S0hOS7pb0Znd/uIBytVqdtxAo4krCacuffJxB6ACANsnbtXe9pEvcfY+ZfVDSJZLenb9YyCKkkBJCGdoqpO0MAF2XK0i5+5cS/94i6bX5itMNeVqFQrj3UhFfkAwAQBsUOdj8LZI2FTg/TIi7jXcD2xkAwjM2SJnZDZKOSXnqUne/Kp7mUkl7JF05Yj7rJK2TpBUrVmQqbNtkaYkK5SRa9/IBAAjB2CDl7q8Y9byZnS/pHEmnubuPmM9GSRslqdfrDZ0O0yvjq2iaEJSaUMYihfiVQwDQdXmv2jtT0kWSXu7ujxVTJKQJ5SSa1jI2N7/AncoBAJ2Ud4zUn0o6VNL1ZiZJt7j723KXCpnkDTLJUBRKF2Ka0Lo5q9aV9QSAJsh71d6ziyoIJlP3STTZMta/Y/mu3Xu05d6dnQs0AADwFTE4oIVHUtBfLBxKNycAAAQpZMLdygEAIEhBzW3haUo5AQDtRZDCUJMGKwINAKCrCFIlaVrrjtSssgIAEAKCFA7Q9dsLAAAwKYJUwQghAAB0B0EKB2jq4HMAAKpGkCoYIaSZ2F4AgCwIUhiKUAEAwGgEqZIQQkYLpQWIMW0AgDwW1V0AAACApqJFCplkbbkJrQWIMW0AgDxokQIAAMjI3L3yhfZ6PZ+dna18uchvsEVpzarlkrK3TNECBAAInZltdfde2nO0SAEAAGREixQyGdWiRGsTAKBNaJECAAAoAVftIZNRLVGhXJEHAEDZaJECAADIiBYpFIZ7MgEAuoYWKQAAgIxokULhaIkCAHQFLVIAAAAZEaQAAAAyIkgBAABkRJACAADIiCAFAACQEUEKma3dsHnvPaMAAOgighQAAEBG3EcKU+M79QAAiNAiBQAAkBEtUpga36kHAECEFikAAICMaJFCZrREAQC6jhYpAACAjAoJUmb2TjNzMzuqiPkBAAA0Qe4gZWbHSTpd0vfyFwcAAKA5imiR+mNJF0nyAuYFAADQGLmClJmdK2mHu99WUHkAAAAaY+xVe2Z2g6RjUp66VNJ7FHXrjWVm6yStk6QVK1ZMUUQAAIAwmXu2Hjkze56kL0t6LH7oWEk/kPRid39g1Gt7vZ7Pzs5mWi4AAECVzGyru/fSnst8Hyl33ybp6YmFbJfUc/cfZZ0nAABAk3AfKQAAgIwyd+3lWqjZQ5Luq2BRR0mihWw46mc86mg06mc86mg06mc86mi0KurneHd/WtoTtQSpqpjZ7LA+TVA/k6CORqN+xqOORqN+xqOORqu7fujaAwAAyIggBQAAkFHbg9TGugsQOOpnPOpoNOpnPOpoNOpnPOpotFrrp9VjpAAAAMrU9hYpAACA0rQqSJnZejPbYWbfjH/OHjLdmWZ2h5ndZWYXV13OupjZH5rZd83sW2b2eTM7Ysh0281sW1yHrb8F/bj9wcwONbNN8fNbzGxlDcWsjZkdZ2ZfNbM5M/uOmb09ZZpTzOyRxHvvvXWUtU7j3jcW+XC8H33LzE6so5x1MLPnJPaNb5rZgpm9Y2Cazu1DZvYxM3vQzL6deGy5mV1vZnfGv48c8to3xdPcaWZvqq7U1RlSP+Gdx9y9NT+S1kt615hpDpJ0t6RfkHSIpNskra677BXVz+mSFsd/f1DSB4dMt13SUXWXt6I6Gbs/SPpPkj4a/32epE11l7viOpqRdGL891JJ/5RSR6dI+kLdZa25nka+bySdLelaSSbpJZK21F3mmurpIEkPKLovT/Lxzu1Dkl4m6URJ30489r8kXRz/fXHacVrSckn3xL+PjP8+su71qah+gjuPtapFakIvlnSXu9/j7k9I+oykc2suUyXc/Uvuvif+9xZF34/YdZPsD+dK+kT892clnWZmVmEZa+Xu8+5+a/z3Lkm3S3pmvaVqpHMlfdIjt0g6wsxm6i5UDU6TdLe7V3FT5qC5+z9I2jnwcPJ48wlJr0556RmSrnf3ne7+Y0nXSzqzrHLWJa1+QjyPtTFIXRg3+X1sSJPoMyV9P/H//ermSeEtij4dp3FJXzKzrWa2rsIy1WGS/WHvNPEb+BFJT62kdIGJuzVfKGlLytMnmdltZnatmT232pIFYdz7hmNP5DxJnx7yXNf3IUk62t3n478fkHR0yjTsS5EgzmOZv7S4LmZ2g6RjUp66VNKfS3qfogp8n6QPKarozhhVP+5+VTzNpZL2SLpyyGxe6u47zOzpkq43s+/GnwzQYWb2FEmfk/QOd18YePpWRV01j8ZjE/9O0gkVF7FuvG/GMLNDJL1K0iUpT7MPDXB3NzMurU8R0nmscUHK3V8xyXRm9heSvpDy1A5JxyX+PzZ+rBXG1Y+ZnS/pHEmnedyRnDKPHfHvB83s84q6v9p6Qphkf+hPc7+ZLZZ0uKR/rqZ4YTCzgxWFqCvd/W8Hn08GK3e/xsz+zMyOcvfOfD/YBO+bVh97JnSWpFvd/YeDT7AP7fVDM5tx9/m46/fBlGl2KBpT1nespBsrKFsQQjuPtaprb2C8wWskfTtlsq9LOsHMVsWfjs6TdHUV5aubmZ0p6SJJr3L3x4ZM82QzW9r/W9HAvrR6bItJ9oerJfWvinmtpK8Me/O2UTwe7C8l3e7ufzRkmmP648bM7MWKji2dCZsTvm+ulvTG+Oq9l0h6JNGF0xW/rSHdel3fhxKSx5s3SboqZZrrJJ1uZkfGQ1hOjx9rvSDPY3WOyC/6R9JfS9om6VuKdsaZ+PFnSLomMd3Ziq48ultRl1ftZa+ofu5S1K/+zfinfyXa3vpRdPXabfHPd7pQP2n7g6TfV/RGlaQlkv5PXH9fk/QLdZe54vp5qaLu8m8l9p2zJb1N0tviaS6M95fbFA0A/dW6y11xHaW+bwbqyCR9JN7Ptknq1V3uiuvoyYqC0eGJxzq9DykKlfOSfqponNNbFY2//LKkOyXdIGl5PG1P0hWJ174lPibdJenNda9LhfUT3HmMO5sDAABk1KquPQAAgCoRpAAAADIiSAEAAGREkAIAAMiIIAUAAJARQQoAACAjghQAAEBGBCkAAICM/j8AtYFUn5JOBwAAAABJRU5ErkJggg==","text/plain":["<Figure size 720x360 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Graficamos los datos:\n","plt.figure(figsize=(10, 5))\n","plt.scatter(data[:, 0], data[:, 1], marker='+')\n","plt.title('Datos a clasificar', fontsize=20)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["Empezaremos por agrupar los puntos de datos aleatorios que acabamos de crear. Ocuparemos la clase AgglomerativeClustering.\n","\n","Esta clase toma dos parametros:\n","\n","<li><strong>n_clusters:</strong> Es el número de clusters a formar así como el número de centroides a generar. Como se ha mencionado el número de grupos será de 4.</li>\n","<li><strong>linkage:</strong> Es el criterio de vinculación a utilizar. El criterio de vinculación determina la distancia a utilizar entre los conjuntos de observación. El algoritmo fusionará los elementos de cluster que minimicen este criterio. En este caso el valor será 'average'. Existen otros metodos que son: (‘ward’, ‘complete’, ‘average’, ‘single’) sientase libre de cambiar estos valores.</li> \n","\n","Finalmente guardamos el resultado en una variable, en este caso \"clus_agg\".\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["# Definimos el modelo de agrupamiento:\n","cluss_agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n"]},{"cell_type":"markdown","metadata":{},"source":["Entrenamos el modelo para los datos generados previamente (data, labels)."]},{"cell_type":"code","execution_count":41,"metadata":{"scrolled":true},"outputs":[{"data":{"text/plain":["AgglomerativeClustering(linkage='average', n_clusters=4)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# Entrenamos el modelo:\n","cluss_agg.fit(data, labels)\n"]},{"cell_type":"markdown","metadata":{},"source":["Con esto, el modelo estaría terminado. Vamos a generar un gráfico para ver cómo se ha generado la agrupación.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["<!--aqui quede-->\n","# Create a figure of size 6 inches by 4 inches.\n","plt.figure(figsize=(6,4))\n","\n","# These two lines of code are used to scale the data points down,\n","# Or else the data points will be scattered very far apart.\n","\n","# Create a minimum and maximum range of X1.\n","x_min, x_max = np.min(X1, axis=0), np.max(X1, axis=0)\n","\n","# Get the average distance for X1.\n","X1 = (X1 - x_min) / (x_max - x_min)\n","\n","# This loop displays all of the datapoints.\n","for i in range(X1.shape[0]):\n","    # Replace the data points with their respective cluster value \n","    # (ex. 0) and is color coded with a colormap (plt.cm.spectral)\n","    plt.text(X1[i, 0], X1[i, 1], str(y1[i]),\n","             color=plt.cm.nipy_spectral(agglom.labels_[i] / 10.),\n","             fontdict={'weight': 'bold', 'size': 9})\n","    \n","# Remove the x ticks, y ticks, x and y axis\n","plt.xticks([])\n","plt.yticks([])\n","#plt.axis('off')\n","\n","\n","\n","# Display the plot of the original data before clustering\n","plt.scatter(X1[:, 0], X1[:, 1], marker='.')\n","# Display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<h3 id=\"dendrogram\">Dendrogram Associated for the Agglomerative Hierarchical Clustering</h3>\n","\n","Remember that a <b>distance matrix</b> contains the <b> distance from each point to every other point of a dataset </b>.\n","\n","Use the function <b> distance_matrix, </b> which requires <b>two inputs</b>. Use the Feature Matrix, <b> X1 </b> as both inputs and save the distance matrix to a variable called <b> dist_matrix </b> <br> <br>\n","Remember that the distance values are symmetric, with a diagonal of 0's. This is one way of making sure your matrix is correct. <br> (print out dist_matrix to make sure it's correct)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["dist_matrix = distance_matrix(X1,X1) \n","print(dist_matrix)"]},{"cell_type":"markdown","metadata":{},"source":["Using the <b> linkage </b> class from hierarchy, pass in the parameters:\n","\n","<ul>\n","    <li> The distance matrix </li>\n","    <li> 'complete' for complete linkage </li>\n","</ul> <br>\n","Save the result to a variable called <b> Z </b>.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Z = hierarchy.linkage(dist_matrix, 'complete')"]},{"cell_type":"markdown","metadata":{},"source":["A Hierarchical clustering is typically visualized as a dendrogram as shown in the following cell. Each merge is represented by a horizontal line. The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where cities are viewed as singleton clusters.\n","By moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering.\n","\n","Next, we will save the dendrogram to a variable called <b>dendro</b>. In doing this, the dendrogram will also be displayed.\n","Using the <b> dendrogram </b> class from hierarchy, pass in the parameter:\n","\n","<ul> <li> Z </li> </ul>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dendro = hierarchy.dendrogram(Z)"]},{"cell_type":"markdown","metadata":{},"source":["## Practice\n","\n","We used **complete** linkage for our case, change it to **average** linkage to see how the dendogram changes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# write your code here\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","Z = hierarchy.linkage(dist_matrix, 'average')\n","dendro = hierarchy.dendrogram(Z)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{},"source":["<hr>\n","<h1 id=\"clustering_vehicle_dataset\">Clustering on Vehicle dataset</h1>\n","\n","Imagine that an automobile manufacturer has developed prototypes for a new vehicle. Before introducing the new model into its range, the manufacturer wants to determine which existing vehicles on the market are most like the prototypes--that is, how vehicles can be grouped, which group is the most similar with the model, and therefore which models they will be competing against.\n","\n","Our objective here, is to use clustering methods, to find the most distinctive clusters of vehicles. It will summarize the existing vehicles and help manufacturers to make decision about the supply of new models.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Download data\n","\n","To download the data, we will use **`!wget`** to download it from IBM Object Storage.\\\n","**Did you know?** When it comes to Machine Learning, you will likely be working with large datasets. As a business, where can you host your data? IBM is offering a unique opportunity for businesses, with 10 Tb of IBM Cloud Object Storage: [Sign up now for free](http://cocl.us/ML0101EN-IBM-Offer-CC)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!wget -O cars_clus.csv https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%204/data/cars_clus.csv"]},{"cell_type":"markdown","metadata":{},"source":["## Read data\n","\n","Let's read dataset to see what features the manufacturer has collected about the existing models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filename = 'cars_clus.csv'\n","\n","#Read csv\n","pdf = pd.read_csv(filename)\n","print (\"Shape of dataset: \", pdf.shape)\n","\n","pdf.head(5)"]},{"cell_type":"markdown","metadata":{},"source":["The feature sets include  price in thousands (price), engine size (engine_s), horsepower (horsepow), wheelbase (wheelbas), width (width), length (length), curb weight (curb_wgt), fuel capacity (fuel_cap) and fuel efficiency (mpg).\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2 id=\"data_cleaning\">Data Cleaning</h2>\n","\n","Let's clean the dataset by dropping the rows that have null value:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print (\"Shape of dataset before cleaning: \", pdf.size)\n","pdf[[ 'sales', 'resale', 'type', 'price', 'engine_s',\n","       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n","       'mpg', 'lnsales']] = pdf[['sales', 'resale', 'type', 'price', 'engine_s',\n","       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n","       'mpg', 'lnsales']].apply(pd.to_numeric, errors='coerce')\n","pdf = pdf.dropna()\n","pdf = pdf.reset_index(drop=True)\n","print (\"Shape of dataset after cleaning: \", pdf.size)\n","pdf.head(5)"]},{"cell_type":"markdown","metadata":{},"source":["### Feature selection\n","\n","Let's select our feature set:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["featureset = pdf[['engine_s',  'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg']]"]},{"cell_type":"markdown","metadata":{},"source":["### Normalization\n","\n","Now we can normalize the feature set. **MinMaxScaler** transforms features by scaling each feature to a given range. It is by default (0, 1). That is, this estimator scales and translates each feature individually such that it is between zero and one.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","x = featureset.values #returns a numpy array\n","min_max_scaler = MinMaxScaler()\n","feature_mtx = min_max_scaler.fit_transform(x)\n","feature_mtx [0:5]"]},{"cell_type":"markdown","metadata":{},"source":["<h2 id=\"clustering_using_scipy\">Clustering using Scipy</h2>\n","\n","In this part we use Scipy package to cluster the dataset.\n","\n","First, we calculate the distance matrix.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import scipy\n","leng = feature_mtx.shape[0]\n","D = scipy.zeros([leng,leng])\n","for i in range(leng):\n","    for j in range(leng):\n","        D[i,j] = scipy.spatial.distance.euclidean(feature_mtx[i], feature_mtx[j])\n","D"]},{"cell_type":"markdown","metadata":{},"source":["In agglomerative clustering, at each iteration, the algorithm must update the distance matrix to reflect the distance of the newly formed cluster with the remaining clusters in the forest.\n","The following methods are supported in Scipy for calculating the distance between the newly formed cluster and each:\n","\\- single\n","\\- complete\n","\\- average\n","\\- weighted\n","\\- centroid\n","\n","We use **complete** for our case, but feel free to change it to see how the results change.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pylab\n","import scipy.cluster.hierarchy\n","Z = hierarchy.linkage(D, 'complete')"]},{"cell_type":"markdown","metadata":{},"source":["Essentially, Hierarchical clustering does not require a pre-specified number of clusters. However, in some applications we want a partition of disjoint clusters just as in flat clustering.\n","So you can use a cutting line:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.cluster.hierarchy import fcluster\n","max_d = 3\n","clusters = fcluster(Z, max_d, criterion='distance')\n","clusters"]},{"cell_type":"markdown","metadata":{},"source":["Also, you can determine the number of clusters directly:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.cluster.hierarchy import fcluster\n","k = 5\n","clusters = fcluster(Z, k, criterion='maxclust')\n","clusters\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, plot the dendrogram:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = pylab.figure(figsize=(18,50))\n","def llf(id):\n","    return '[%s %s %s]' % (pdf['manufact'][id], pdf['model'][id], int(float(pdf['type'][id])) )\n","    \n","dendro = hierarchy.dendrogram(Z,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')"]},{"cell_type":"markdown","metadata":{},"source":["<h2 id=\"clustering_using_skl\">Clustering using scikit-learn</h2>\n","\n","Let's redo it again, but this time using the scikit-learn package:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics.pairwise import euclidean_distances\n","dist_matrix = euclidean_distances(feature_mtx,feature_mtx) \n","print(dist_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Z_using_dist_matrix = hierarchy.linkage(dist_matrix, 'complete')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = pylab.figure(figsize=(18,50))\n","def llf(id):\n","    return '[%s %s %s]' % (pdf['manufact'][id], pdf['model'][id], int(float(pdf['type'][id])) )\n","    \n","dendro = hierarchy.dendrogram(Z_using_dist_matrix,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')"]},{"cell_type":"markdown","metadata":{},"source":["Now, we can use the 'AgglomerativeClustering' function from scikit-learn library to cluster the dataset. The AgglomerativeClustering performs a hierarchical clustering using a bottom up approach. The linkage criteria determines the metric used for the merge strategy:\n","\n","*   Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n","*   Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n","*   Average linkage minimizes the average of the distances between all observations of pairs of clusters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')\n","agglom.fit(dist_matrix)\n","\n","agglom.labels_"]},{"cell_type":"markdown","metadata":{},"source":["We can add a new field to our dataframe to show the cluster of each row:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pdf['cluster_'] = agglom.labels_\n","pdf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.cm as cm\n","n_clusters = max(agglom.labels_)+1\n","colors = cm.rainbow(np.linspace(0, 1, n_clusters))\n","cluster_labels = list(range(0, n_clusters))\n","\n","# Create a figure of size 6 inches by 4 inches.\n","plt.figure(figsize=(16,14))\n","\n","for color, label in zip(colors, cluster_labels):\n","    subset = pdf[pdf.cluster_ == label]\n","    for i in subset.index:\n","            plt.text(subset.horsepow[i], subset.mpg[i],str(subset['model'][i]), rotation=25) \n","    plt.scatter(subset.horsepow, subset.mpg, s= subset.price*10, c=color, label='cluster'+str(label),alpha=0.5)\n","#    plt.scatter(subset.horsepow, subset.mpg)\n","plt.legend()\n","plt.title('Clusters')\n","plt.xlabel('horsepow')\n","plt.ylabel('mpg')"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, we are seeing the distribution of each cluster using the scatter plot, but it is not very clear where is the centroid of each cluster. Moreover, there are 2 types of vehicles in our dataset, \"truck\" (value of 1 in the type column) and \"car\" (value of 0 in the type column). So, we use them to distinguish the classes, and summarize the cluster. First we count the number of cases in each group:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pdf.groupby(['cluster_','type'])['cluster_'].count()"]},{"cell_type":"markdown","metadata":{},"source":["Now we can look at the characteristics of each cluster:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["agg_cars = pdf.groupby(['cluster_','type'])['horsepow','engine_s','mpg','price'].mean()\n","agg_cars"]},{"cell_type":"markdown","metadata":{},"source":["It is obvious that we have 3 main clusters with the majority of vehicles in those.\n","\n","**Cars**:\n","\n","*   Cluster 1: with almost high mpg, and low in horsepower.\n","\n","*   Cluster 2: with good mpg and horsepower, but higher price than average.\n","\n","*   Cluster 3: with low mpg, high horsepower, highest price.\n","\n","**Trucks**:\n","\n","*   Cluster 1: with almost highest mpg among trucks, and lowest in horsepower and price.\n","*   Cluster 2: with almost low mpg and medium horsepower, but higher price than average.\n","*   Cluster 3: with good mpg and horsepower, low price.\n","\n","Please notice that we did not use **type**  and **price** of cars in the clustering process, but Hierarchical clustering could forge the clusters and discriminate them with quite a high accuracy.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(16,10))\n","for color, label in zip(colors, cluster_labels):\n","    subset = agg_cars.loc[(label,),]\n","    for i in subset.index:\n","        plt.text(subset.loc[i][0]+5, subset.loc[i][2], 'type='+str(int(i)) + ', price='+str(int(subset.loc[i][3]))+'k')\n","    plt.scatter(subset.horsepow, subset.mpg, s=subset.price*20, c=color, label='cluster'+str(label))\n","plt.legend()\n","plt.title('Clusters')\n","plt.xlabel('horsepow')\n","plt.ylabel('mpg')\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Want to learn more?</h2>\n","\n","IBM SPSS Modeler is a comprehensive analytics platform that has many machine learning algorithms. It has been designed to bring predictive intelligence to decisions made by individuals, by groups, by systems – by your enterprise as a whole. A free trial is available through this course, available here: <a href=\"https://www.ibm.com/analytics/spss-statistics-software?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\">SPSS Modeler</a>\n","\n","Also, you can use Watson Studio to run these notebooks faster with bigger datasets. Watson Studio is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, Watson Studio enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of Watson Studio users today with a free account at <a href=\"https://www.ibm.com/cloud/watson-studio?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\">Watson Studio</a>\n"]},{"cell_type":"markdown","metadata":{},"source":["### Thank you for completing this lab!\n","\n","## Author\n","\n","Saeed Aghabozorgi\n","\n","### Other Contributors\n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n","\n","## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                  |\n","| ----------------- | ------- | ---------- | --------------------------------------------------- |\n","| 2021-01-11        | 2.2     | Lakshmi    | Changed distance matrix in agglomerative clustering |\n","| 2020-11-03        | 2.1     | Lakshmi    | Updated URL                                         |\n","| 2020-08-27        | 2.0     | Lavanya    | Moved lab to course repo in GitLab                  |\n","\n","## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":2}
